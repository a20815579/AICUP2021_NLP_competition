# -*- coding: utf-8 -*-

import json
import unicodedata
import re
import tqdm
import math
import re
import os
import gc
import random
import pandas as pd
import numpy as np

import torch
import torch.nn.functional as F
from torch import nn
from torch.nn import CrossEntropyLoss
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler

from google.colab import drive

import transformers
from transformers import BertPreTrainedModel, BertModel, BertConfig
from transformers import BertTokenizer
from transformers import BertForMultipleChoice
from transformers import LongformerTokenizer
from transformers import LongformerForMultipleChoice
from transformers.modeling_outputs import MultipleChoiceModelOutput

from datasets import load_metric

from IPython.display import clear_output

drive.mount('/content/drive')

# helper
random_seed = 42
random.seed(random_seed)
np.random.seed(random_seed)
torch.manual_seed(random_seed)

PRETRAINED_MODEL_NAME = "bert-base-chinese"
#PRETRAINED_MODEL_NAME = "hfl/chinese-roberta-wwm-ext"
#PRETRAINED_MODEL_NAME = 'voidful/albert_chinese_base'

tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)

clear_output()
print("PyTorch ç‰ˆæœ¬ï¼š", torch.__version__)

with open("./drive/MyDrive/Aicup/Train_qa_ans.json", "r", encoding = "utf-8") as f_QA:
    data = json.load(f_QA)

train_df = pd.json_normalize(data)

with open("./drive/MyDrive/Aicup/Develop_QA.json", "r", encoding = "utf-8") as f_QA:
    data = json.load(f_QA)

test_df = pd.json_normalize(data)

df = pd.concat([train_df,test_df], axis=0)
train_df_length = len(train_df)


print(f"max article length: {max([len(i) for i in test_df['text']])}")
print(f"max question length: {max([len(i) for i in test_df['question.stem']])}")
print()
print(train_df.answer.value_counts())

# Fix answer error (remove space and error label)
def fix_answer(x):
    if type(x.answer) is float:
        return x
    n = x.answer.replace(" ", "")
    n = unicodedata.normalize("NFKC", n)
    if n not in ['A', 'B', 'C']:
        for c in x["question.choices"]:
            if c["text"] == n:
                n = c["label"]
                break
        else:
            global w
            w = x.answer
            print(x.answer)
            print(x.answer in ['A', 'B', 'C'])
    x.answer = n
    return x
# lower and unify
def fix_unicode(x):
    x.text = unicodedata.normalize("NFKC", x.text.lower())
    x.question = unicodedata.normalize("NFKC", x.question.lower())
    x.choiceA = unicodedata.normalize("NFKC", x.choiceA.lower())
    x.choiceB = unicodedata.normalize("NFKC", x.choiceB.lower())
    x.choiceC = unicodedata.normalize("NFKC", x.choiceC.lower())
    return x

df = df.apply(fix_answer, axis=1)
rows = {"choiceA":[], "choiceB":[], "choiceC":[]}
for index, row in df.iterrows():
    rows['choiceA'].append(row['question.choices'][0]['text'])
    rows['choiceB'].append(row['question.choices'][1]['text'])
    rows['choiceC'].append(row['question.choices'][2]['text'])
df["choiceA"] = rows["choiceA"]
df["choiceB"] = rows["choiceB"]
df["choiceC"] = rows["choiceC"]
df = df.drop("question.choices", axis=1)
df = df.rename(columns={'question.stem': 'question'})
df = df.apply(fix_unicode, axis=1)

"""## è³‡æ–™è™•ç†"""

def get_talker_name():
    talker = set()

    for idx, row in df.iterrows():
        talker = talker.union(set([row.text[m.start()-2:m.start()] for m in re.finditer(':', row.text)]))

    return talker

print(get_talker_name())

def replace_name(row):
    row.text = row.text.replace('é†«å¸«a', 'é†«å¸«')
    row.text = row.text.replace('é†«å¸«b', 'é†«å¸«')
    row.text = row.text.replace('å®¶å±¬a', 'å®¶å±¬')
    row.text = row.text.replace('å®¶å±¬b', 'å®¶å±¬')
    row.text = row.text.replace('å®¶å±¬1', 'å®¶å±¬')
    row.text = row.text.replace('å®¶å±¬2', 'å®¶å±¬')
    row.text = row.text.replace('å®¶å±¬ã€b', 'å®¶å±¬')
    row.text = row.text.replace('æ°‘çœ¾a', 'æ°‘çœ¾')
    row.text = row.text.replace('æ°‘çœ¾ï¼šæˆ‘æ˜¯82ã€‚ç”Ÿ', 'æ°‘çœ¾')
    row.text = row.text.replace('è­·ç†å¸«a', 'è­·ç†å¸«')
    row.text = row.text.replace('è­·ç†å¸«b', 'è­·ç†å¸«')
    row.text = row.text.replace('è­·ç†å¸«c', 'è­·ç†å¸«')
    row.text = row.text.replace('å€‹ç®¡å¸«', 'é†«å¸«')
    row.text = row.text.replace('ä¸ç¢ºå®šäººç‰©', 'é†«å¸«')
    row.text = re.sub("[ab]å…ˆç”Ÿ","å…ˆç”Ÿ", row.text)
    row.text = re.sub('[é˜¿å•Šç—¾å•¦å‘€å•¦å˜›å“‡å§ã„Ÿæ¬¸è¯¶èª’è€¶é¤’å’§å˜å‹’äº†å—å‘¢å””]', '', row.text)
    row.text = re.sub('[å“¼äº¨è›¤è¦å‘ƒå‘µå“ˆå˜¿å–”å“¦å”·å‘¦å–²é½å•¦æ‘æ©å—¯å“å”‰]', '', row.text)
    row.text = row.text.replace('æŒ‚', 'æ›')
    row.text = row.text.replace('å¦‚æœå‡è¨­', 'å¦‚æœ')
    row.text = row.text.replace('è‰¾æ»‹ç—…', 'æ„›æ»‹ç—…')
    row.text = row.text.replace('\u200b', '')
    row.text = re.sub('å¥½+', 'å¥½', row.text)
    row.text = re.sub('æˆ‘+', 'æˆ‘', row.text)
    row.text = re.sub('å°+', 'å°', row.text)    
    row.text = re.sub('æœƒ+', 'æœƒ', row.text)
    row.text = re.sub('è¬+', 'è¬', row.text)
    row.text = re.sub('åˆ+', 'åˆ', row.text)
    row.text = re.sub('éƒ½+', 'éƒ½', row.text)
    row.text = re.sub('ä¸+', 'ä¸', row.text)
    row.text = re.sub('æ˜¯+', 'æ˜¯', row.text)
    #row.text = re.sub("é†«å¸«[AB]?ï¼š","", row.text) 
    #row.text = re.sub("å®¶å±¬[B12]?ï¼š","", row.text) 
    #row.text = re.sub("æ°‘çœ¾A?ï¼š","", row.text)
    #row.text = re.sub("è­·ç†å¸«[AB]?ï¼š","", row.text)
    #row.text = row.text.replace('å€‹ç®¡å¸«ï¼š', '')
    
    row.text = row.text.replace('â€¦', '')
    row.text = row.text.replace('â‹¯', '') 
    row.text = row.text.replace('Â·', '')
    row.text = re.sub('[é˜¿å•Šç—¾å•¦å‘€å•¦å˜›å§]', '', row.text)
    row.text = re.sub('[ã„Ÿæ¬¸è¯¶èª’é¤’]', '', row.text)
    row.text = re.sub('[å“¼äº¨]', '', row.text)
    row.text = re.sub("[è›¤è¦]ï¼Ÿ?", "", row.text)
    row.text = re.sub('[å–”å“¦å”·å‘¦]', '', row.text)
    row.text = re.sub('[å‘ƒå‘µå“ˆå˜¿]', '', row.text)
    row.text = re.sub('[(OK)(ok)å¥½]+', 'å¥½', row.text)
    row.text = re.sub('å°+', 'å°', row.text)
    row.text = re.sub('[æ©å—¯]+', 'å—¯', row.text)
    row.text = re.sub('æœƒ+', 'æœƒ', row.text)
    row.text = re.sub('è¬è¬+', 'è¬', row.text)
    row.text = row.text.replace('ï½', '')
    #row.text = re.sub('[(ç„¶å¾Œ)(æ‰€ä»¥)(è€Œä¸”)]', '', row.text)
    #row.text = re.sub('[(å¦‚æœ)(é‚„æ˜¯)(å› ç‚º)]', '', row.text) # check
    row.text = re.sub('[å—çš„å—¯]', '', row.text) # check
    row.text = row.text.replace('å¦‚æœå‡è¨­', 'å¦‚æœ')
    #row.text = row.text.replace('', '')
    row.text = re.sub("å°±æ˜¯?", "", row.text)
    row.text = re.sub("é€™æ¨£å­?[ã€‚ï¼Œ]", "", row.text)
    row.text = re.sub("[å¥½å—¯][ã€‚ï¼Œï¼Ÿ][å¥½å—¯][ã€‚ï¼Œï¼Ÿ]", "å¥½ï¼Œ", row.text)
    row.text = row.text.replace('ï¼Œã€‚', 'ã€‚')
    row.text = row.text.replace('ã€‚ï¼Œ', 'ã€‚')
    row.text = row.text.replace('ï¼Œï¼Œ', 'ï¼Œ')
    row.text = row.text.replace('ã€‚ã€‚', 'ã€‚')
    row.text = re.sub('ï¼Ÿ[ã€‚ï¼Œ]', 'ï¼Ÿ', row.text)
    row.text = re.sub('[ã€‚ï¼Œ]ï¼Ÿ', 'ï¼Ÿ', row.text)
    row.text = re.sub("[ã€‚ï¼Œï¼Ÿ]", '', row.text) # check
    row.text = re.sub("å¸¶å¥—", 'æˆ´å¥—', row.text)
    #row.text = re.sub("ä¸€", '1', row.text)
    #row.text = re.sub("äºŒ", '2', row.text)
    #row.text = re.sub("ä¸‰", '3', row.text)
    #row.text = re.sub("å››", '4', row.text)
    #row.text = re.sub("äº”", '5', row.text)
    #row.text = re.sub("å…­", '6', row.text)
    #row.text = re.sub("ä¸ƒ", '7', row.text)
    #row.text = re.sub("å…«", '8', row.text)
    #row.text = re.sub("ä¹", '9', row.text)
    #row.text = re.sub("åé»", '10é»', row.text)
    #row.text = re.sub("å", '', row.text)

    row.text = re.sub("ç ²", 'ç‚®', row.text)
    row.question = re.sub("ç ²", 'ç‚®', row.question)
    row.choiceA = re.sub("ç ²", 'ç‚®', row.choiceA)
    row.choiceB = re.sub("ç ²", 'ç‚®', row.choiceB)
    row.choiceC = re.sub("ç ²", 'ç‚®', row.choiceC)
    row.question = re.sub("æœ‰èª¤", 'éŒ¯èª¤', row.question)
    row.question = re.sub("ä½•é€™éŒ¯èª¤", 'éŒ¯èª¤', row.question)
    row.question = re.sub("ä½•è€…ç‚ºçœŸ", 'ä½•è€…æ­£ç¢º', row.question)
    row.question = re.sub("ä½•é …", 'ä½•è€…', row.question)
    row.question = re.sub("è«‹å•", '', row.question)
    row.question = re.sub("q1.", '', row.question)
    return row

df = df.apply(replace_name, axis=1)

print(get_talker_name())

# run these to avoid OOM
def clear_mm():
    gc.collect()
    torch.cuda.empty_cache()

import jieba
import synonyms
import random
from random import shuffle

random.seed(2019)

import jieba
import jieba.analyse

jieba.set_dictionary('dict.txt.big')
jieba.analyse.set_stop_words('stopwords.txt')

jieba.add_word('å€‹ç®¡å¸«')
jieba.add_word('é€™å€‹æœˆ')
jieba.add_word('æ€§è¡Œç‚º')
jieba.add_word('ç„¡å¥—')
jieba.add_word('ä¿éšªå¥—')
jieba.add_word('å›ºç‚®')
jieba.add_word('ç´„ç‚®')
jieba.add_word('æ€§ç”Ÿæ´»')
jieba.add_word('å°æ™‚')
jieba.add_word('å…§å°„')
jieba.add_word('69')
jieba.add_word('10') # ###
jieba.add_word('ç”²ç‹€è…º')
jieba.add_word('ç™¼ç‡’')
jieba.add_word('å™´åš')
jieba.add_word('å›è¨º')
jieba.add_word('å£äº¤')
jieba.add_word('æ¢…æ¯’')
jieba.add_word('æŠ—é«”')
jieba.add_word('æ´—è…')
jieba.add_word('hpv')
jieba.add_word('prep')
jieba.add_word('hiv')
jieba.add_word('å…ç–«')
jieba.add_word('èœèŠ±')
jieba.add_word('è—¥')
jieba.add_word('ä¼´ä¾¶')
jieba.add_word('é™½æ€§')
#jieba.add_word('å›ºå®šæ€§ä¼´ä¾¶')
jieba.add_word('ä»»å‹™å‹')
jieba.add_word('æ„ŸæŸ“')
jieba.add_word('å¥ä¿å¡')
jieba.add_word('æª¢é©—')
jieba.add_word('æŠ—é«”')
jieba.add_word('æ‚£è€…')
jieba.add_word('è‚›äº¤')
jieba.add_word('æ„›æ»‹')
jieba.add_word('é–‹æ”¾å¼é—œä¿‚')
jieba.add_word('å¸¶åŸ')
jieba.add_word('å°ä¸‰')
jieba.add_word('å£çˆ†')
jieba.add_word('ç™½è¡€çƒ')
jieba.add_word('ç´…è¡€çƒ')
jieba.add_word('è¡€å°æ¿')
jieba.add_word('ç´…è¡€çƒ')
jieba.add_word('é—œæ–¼')
jieba.add_word('å»ºè­°')
jieba.add_word('è—¥')
jieba.add_word('å‚³æŸ“ç—…')
jieba.add_word('å°æ–¼')
jieba.add_word('åŸå» è—¥')
jieba.add_word('å­¸åè—¥')
jieba.add_word('æ°‘çœ¾')
jieba.add_word('bå‹è‚ç‚')
jieba.add_word('åŒ¿ç¯©')
jieba.add_word('ç¯©æª¢')
jieba.add_word('æ€§é—œä¿‚')
jieba.add_word('å¾ªç’°')
jieba.add_word('è¿½è¹¤')
jieba.add_word('xå…‰')
jieba.add_word('é¼»ç«‡ç‚')
jieba.add_word('cd4')
jieba.add_word('cè‚')
jieba.add_word('èƒƒç—™æ”£')
jieba.add_word('ç‰¹æ•ˆè—¥')
jieba.add_word('å‚³çµ±')
jieba.add_word('è…æŒ‡æ•¸')
jieba.add_word('ä½•ç¨®')
jieba.add_word('ç‰™é½’')
jieba.add_word('åˆ®é¬åˆ€')
jieba.add_word('bè‚')
jieba.add_word('aè‚')
jieba.add_word('cè‚')
jieba.add_word('cå‹è‚ç‚')
jieba.add_word('5é»')
jieba.add_word('å®‰å…¨æ€§è¡Œç‚º')
jieba.add_word('äº‹å¾Œ')
jieba.add_word('å¤§æ–¼')
jieba.add_word('è†½å›ºé†‡')
jieba.add_word('9é»')
jieba.add_word('èƒƒé£Ÿé“')
jieba.add_word('æ¾±ç²‰')
jieba.add_word('å°ä¾¿')
jieba.add_word('å¾©å¥')
jieba.add_word('å‰ä»»')
jieba.add_word('æ­¢ç—›è—¥')
jieba.add_word('å”¸æ›¸')
jieba.add_word('æ‰“ç ²')
jieba.add_word('æ»·è‚‰é£¯')
jieba.add_word('è¤‡åˆæ€§')
jieba.add_word('çµæ ¸èŒ')
jieba.add_word('bç¾¤')
jieba.add_word('ä¸æˆ´')
jieba.add_word('å™å¿ƒæ„Ÿ')
jieba.add_word('6é»')
jieba.add_word('æœ‰é—œ')
jieba.add_word('è‚')
jieba.add_word('è…')
jieba.add_word('æ™®å¿ƒå¯§')
jieba.add_word('åä¸‰åƒ¹')
jieba.add_word('é ­')
jieba.add_word('æœƒç—›')
jieba.del_word('çš„è—¥')
jieba.del_word('æ€§è¡Œ')
jieba.del_word('è¨‚æœƒ')
jieba.del_word('è—¥å°±')
jieba.add_word('u=u') # wtf

def split_sent(sentence: str):
    first_role_idx = re.search(':', sentence).end(0)
    #out = [sentence[:first_role_idx]]
    out = ['']
    tmp = sentence[first_role_idx:]
    while tmp:
        res = re.search(
            r'(è­·ç†å¸«[\w*]\s*:|é†«å¸«\s*:|æ°‘çœ¾\s*:|å®¶å±¬[\w*]\s*:|å€‹ç®¡å¸«\s*:)', tmp)
        if res is None:
            break
        #å­—æœ¬èº«ä¸èƒ½è¶…é150ï¼ï¼
        idx = res.start(0)
        idx_end = res.end(0)
        f = out[-1] + tmp[:idx]
        if len(f) > 150:
            r = split_sentence(f)
            out[-1] = r[0]
            out += r[1:]
        else:
            out[-1] = f
        out.append('')
        #print(tmp[idx:idx_end])
        #out.append(tmp[idx:idx_end])
        tmp = tmp[idx_end:]

    out[-1] = out[-1] + tmp

    return out

global_tmp = []
def split_df(x):
    sentences = split_sent(x.text)
    tmp = []
    try:
        a = jieba.analyse.extract_tags(x.choiceA, topK=10)
        b = jieba.analyse.extract_tags(x.choiceB, topK=10)
        c = jieba.analyse.extract_tags(x.choiceC, topK=10)
        if len(x.choiceA) <= 2:
            tmp.append(x.choiceA)
        elif len(a) == 0:
            tmp.append(x.choiceA) # check
        else:
            tmp.append(a[0])
        if len(x.choiceB) <= 2:
            tmp.append(x.choiceB) # check
        elif len(b) == 0:
            tmp.append(x.choiceB)
        else:
            tmp.append(b[0])
        if len(x.choiceC) <= 2:
            tmp.append(x.choiceC)
        elif len(c) == 0:
            tmp.append(x.choiceC) # check
        else:
            tmp.append(c[0])
    except:
        print(x['id'])
    global_tmp.append(tmp)
    
    #if len(tmp) == 0:
    #    assert f'{x.idx} no tokens'
    text = []
    addition = False
    for sentence in sentences:
        #sentence = re.sub(r'[:]', 'èªª', sentence)
        sentence = re.sub(r'[^\w\s:]', '', sentence)
        if len(sentence) == 0:
            continue
        if addition:
            text.append(sentence)
            addition = False
            continue
        for match in tmp:
            if match in sentence:
                text.append(sentence)
                addition = True
                break
    if len(text) == 0:
        with open('stopwords.txt') as f:
            stopwords = f.read().splitlines()
        text += [i for i in jieba.lcut(x.choiceA) if i not in stopwords]
        text += [i for i in jieba.lcut(x.choiceA) if i not in stopwords]
        text += [i for i in jieba.lcut(x.choiceA) if i not in stopwords]
    x['new_text'] = ''.join(text)
    return x

df_tmp = df.apply(split_df, axis=1)
df['text'] = df_tmp['new_text']

t_df = df[:len(train_df)]
d_df = df[len(train_df):]

positive = []
negative = []
n_list = ['éŒ¯èª¤', 'é', 'æ²’æœ‰', 'ä¸æ˜¯', 'ä¸æ­£ç¢º', 'ä¸åŒ…å«', 'ä¸ç”¨', 'ä¸éœ€è¦']

for j,i in t_df.iterrows():
    for n in n_list:
        if n in i.question:
            negative.append(j)
            break
    else:
        positive.append(j)
tp_df = t_df.iloc[positive]
tn_df = t_df.iloc[negative]

positive = []
negative = []

for j,i in d_df.iterrows():
    for n in n_list:
        if n in i.question:
            negative.append(j)
            break
    else:
        positive.append(j)

dp_df = d_df.iloc[positive]
dn_df = d_df.iloc[negative]

class InputFeatures(object):
    def __init__(self, example_id, choices_features, label):
        self.example_id = example_id
        self.choices_features = [
            {
                'input_ids': input_ids,
                'input_mask': input_mask,
                'segment_ids': segment_ids
            }
            for _, input_ids, input_mask, segment_ids in choices_features
        ]
        self.label = label

def select_field(features, field):
    return [
        [
            choice[field]
            for choice in feature.choices_features
        ]
        for feature in features
    ]

def convert_examples_to_features(df, tokenizer, max_seq_length=512, is_training=False):
    features = []
    label_map = {'A': 0, 'B': 1, 'C': 2}
    for idx, (_, example) in enumerate(df.iterrows()):
        context_tokens = tokenizer.tokenize(example.text)
        start_ending_tokens = tokenizer.tokenize(example.question)
        
        choices_features = []
        choices = [df.iloc[idx]["choiceA"], df.iloc[idx]["choiceB"], df.iloc[idx]["choiceC"]]
        for ending_index, ending in enumerate(choices):
            # We create a copy of the context tokens in order to be
            # able to shrink it according to ending_tokens
            context_tokens_choice = context_tokens[:]
            ending_tokens = start_ending_tokens + tokenizer.tokenize(ending)
            # Modifies `context_tokens_choice` and `ending_tokens` in
            # place so that the total length is less than the
            # specified length.  Account for [CLS], [SEP], [SEP] with
            # "- 3"
            while True:
                total_length = len(context_tokens_choice) + len(ending_tokens)
                if total_length <= max_seq_length - 3:
                    break
                if len(context_tokens_choice) > len(ending_tokens):
                    context_tokens_choice.pop()
                else:
                    ending_tokens.pop()

            tokens = ["[CLS]"] + context_tokens_choice + ["[SEP]"] + ending_tokens + ["[SEP]"]
            segment_ids = [0] * (len(context_tokens_choice) + 2) + [1] * (len(ending_tokens) + 1)

            input_ids = tokenizer.convert_tokens_to_ids(tokens)
            input_mask = [1] * len(input_ids)

            # Zero-pad up to the sequence length.
            padding = [0] * (max_seq_length - len(input_ids))
            input_ids += padding
            input_mask += padding
            segment_ids += padding

            assert len(input_ids) == max_seq_length
            assert len(input_mask) == max_seq_length
            assert len(segment_ids) == max_seq_length

            choices_features.append((tokens, input_ids, input_mask, segment_ids))

        label = None
        if is_training:
            label = df.iloc[idx]["answer"]
            label = label.replace(" ", "")
            label = unicodedata.normalize("NFKC", label)
            label = label_map[label]

        features.append(
            InputFeatures(
                example_id = example.id,
                choices_features = choices_features,
                label = label
            )
        )
    return features

tp_features = convert_examples_to_features(tp_df, tokenizer, 512, True)
tn_features = convert_examples_to_features(tn_df, tokenizer, 512, True)
dp_features = convert_examples_to_features(dp_df, tokenizer, 512, False)
dn_features = convert_examples_to_features(dn_df, tokenizer, 512, False)

class QADataset(torch.utils.data.Dataset):
    def __init__(self, input_ids, input_mask, segment_ids, labels = None):
        # Dataset class çš„ parameters æ”¾å…¥æˆ‘å€‘ tokenization å¾Œçš„è³‡æ–™ä»¥åŠè³‡æ–™çš„æ¨™ç±¤
        self.input_ids = input_ids
        self.input_mask = input_mask
        self.segment_ids = segment_ids
        self.labels = labels
        self.len = len(input_ids)

    def __getitem__(self, idx):
        # è«‹æ³¨æ„ tokenization å¾Œçš„è³‡æ–™æ˜¯ä¸€å€‹ dict
        # åœ¨æ­¤æ­¥é©Ÿå°‡è³‡æ–™ä»¥åŠæ¨™ç±¤éƒ½è½‰æ›ç‚º PyTorch çš„ tensors
        item = {"input_ids":self.input_ids[idx], "token_type_ids":self.segment_ids[idx], \
                "attention_mask":self.input_mask[idx]}
        if self.labels != None:
          item['labels'] = self.labels[idx]
        return item
        
    def __len__(self):
        # å›å‚³è³‡æ–™é›†çš„ç¸½æ•¸
        return self.len

all_input_ids = torch.tensor(select_field(tp_features, 'input_ids'), dtype=torch.long)
all_input_mask = torch.tensor(select_field(tp_features, 'input_mask'), dtype=torch.long)
all_segment_ids = torch.tensor(select_field(tp_features, 'segment_ids'), dtype=torch.long)
all_label = torch.tensor([f.label for f in tp_features], dtype=torch.long)

dataset = QADataset(all_input_ids, all_input_mask, all_segment_ids, all_label)
train_p_dataset, val_p_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)])

all_input_ids = torch.tensor(select_field(tn_features, 'input_ids'), dtype=torch.long)
all_input_mask = torch.tensor(select_field(tn_features, 'input_mask'), dtype=torch.long)
all_segment_ids = torch.tensor(select_field(tn_features, 'segment_ids'), dtype=torch.long)
all_label = torch.tensor([f.label for f in tn_features], dtype=torch.long)

dataset = QADataset(all_input_ids, all_input_mask, all_segment_ids, all_label)
train_n_dataset, val_n_dataset = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.8), len(dataset) - int(len(dataset) * 0.8)])

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model_p = BertForMultipleChoice.from_pretrained(pretrained_model_name_or_path=PRETRAINED_MODEL_NAME)
model_p = model_p.to(device)
model_n = BertForMultipleChoice.from_pretrained(pretrained_model_name_or_path=PRETRAINED_MODEL_NAME)
model_n = model_n.to(device)

metric = load_metric("accuracy")
def compute_metrics(pred):
    logits, labels = pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)

training_args_p = transformers.TrainingArguments(
    output_dir='./drive/MyDrive/bert_new_keywords_p/result',             # è¼¸å‡ºçš„è³‡æ–™å¤¾
    num_train_epochs=5,               # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®
    per_device_train_batch_size=4,          # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size
    per_device_eval_batch_size=4,          # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size
    #warmup_steps=500,                # learning rate scheduler çš„åƒæ•¸
    #weight_decay=0.01,                # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡
    logging_dir='./drive/MyDrive/bert_new_keywords_p/logs/',              # å­˜æ”¾ log çš„è³‡æ–™å¤¾
    logging_steps=15,
    seed=random_seed,
    learning_rate=1e-5,
    gradient_accumulation_steps = 3,
    eval_accumulation_steps = 3,
    evaluation_strategy = "steps",
)

training_args_n = transformers.TrainingArguments(
    output_dir='./drive/MyDrive/bert_new_keywords_n/result',             # è¼¸å‡ºçš„è³‡æ–™å¤¾
    num_train_epochs=5,               # ç¸½å…±è¨“ç·´çš„ epoch æ•¸ç›®
    per_device_train_batch_size=4,          # è¨“ç·´æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size
    per_device_eval_batch_size=4,          # é©—è­‰æ¨¡å‹æ™‚æ¯å€‹è£ç½®çš„ batch size
    #warmup_steps=500,                # learning rate scheduler çš„åƒæ•¸
    #weight_decay=0.01,                # æœ€ä½³åŒ–æ¼”ç®—æ³• (optimizer) ä¸­çš„æ¬Šé‡è¡°é€€ç‡
    logging_dir='./drive/MyDrive/bert_new_keywords_n/logs/',              # å­˜æ”¾ log çš„è³‡æ–™å¤¾
    logging_steps=15,
    seed=random_seed,
    learning_rate=1e-5,
    gradient_accumulation_steps = 3,
    eval_accumulation_steps = 3,
    evaluation_strategy = "steps",
)

trainer_p = transformers.Trainer(
    model=model_p,                         # ğŸ¤— çš„æ¨¡å‹
    args=training_args_p,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸
    train_dataset=train_p_dataset,      
       # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)
    eval_dataset=val_p_dataset,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰
    compute_metrics=compute_metrics      # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™
)

# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´
# trainer.args._n_gpu=1

# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´
clear_mm()
trainer_p.train()

trainer_p.save_model('./drive/MyDrive/bert_new_keywords_p/model/')

trainer_n = transformers.Trainer(
    model=model_n,                         # ğŸ¤— çš„æ¨¡å‹
    args=training_args_n,                  # Trainer æ‰€éœ€è¦çš„å¼•æ•¸
    train_dataset=train_n_dataset,      
       # è¨“ç·´é›† (æ³¨æ„æ˜¯ PyTorch Dataset)
    eval_dataset=val_n_dataset,            # é©—è­‰é›† (æ³¨æ„æ˜¯ PyTorch Dataset)ï¼Œå¯ä½¿ Trainer åœ¨é€²è¡Œè¨“ç·´æ™‚ä¹Ÿé€²è¡Œé©—è­‰
    compute_metrics=compute_metrics      # è‡ªå®šçš„è©•ä¼°çš„æŒ‡æ¨™
)

# æŒ‡å®šä½¿ç”¨ 1 å€‹ GPU é€²è¡Œè¨“ç·´
# trainer.args._n_gpu=1

# é–‹å§‹é€²è¡Œæ¨¡å‹è¨“ç·´
clear_mm()
trainer_n.train()

trainer_n.save_model('./drive/MyDrive/bert_new_keywords_n/model/')

pred = trainer_n.predict(val_n_dataset)